"""
Attack Interpreter Agent for SecureBank.

This is a read-only security analysis tool that interprets attack patterns from
telemetry data and provides human-readable recommendations for security operators.

IMPORTANT: This module has NO access to:
- Live banking systems
- Customer accounts
- Raw customer PII
- Customer knowledge base or embeddings

It ONLY reads attack metadata and safety classifications from the telemetry database.
"""

import os
import json
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from shared_telemetry import get_telemetry


@dataclass
class AttackEvent:
    """
    Represents a security event detected by observers.

    This contains only metadata about attacks, NOT raw customer PII.
    """
    id: str | int
    timestamp: str
    user_text: str
    agent_text: Optional[str]
    safety_tags: List[str]
    source: str = "interaction"
    risk_score: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            'id': str(self.id),
            'timestamp': self.timestamp,
            'user_text': self.user_text,
            'agent_text': self.agent_text,
            'safety_tags': self.safety_tags,
            'source': self.source,
            'risk_score': self.risk_score
        }


@dataclass
class AttackRecommendation:
    """
    Represents an interpreted attack with recommended defensive actions.

    Generated by LLM analysis of attack metadata (not raw PII).
    """
    event_id: str | int
    intent_summary: str
    risk_level: str  # "LOW" | "MEDIUM" | "HIGH" | "CRITICAL"
    recommended_action: str
    timestamp: str

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            'event_id': str(self.event_id),
            'intent_summary': self.intent_summary,
            'risk_level': self.risk_level,
            'recommended_action': self.recommended_action,
            'timestamp': self.timestamp
        }


# System prompt for the Attack Interpreter LLM
ATTACK_INTERPRETER_SYSTEM_PROMPT = """You are the "Attack Interpreter agent" for SecureBank.

CRITICAL CONSTRAINTS:
- You never access live banking systems, customer accounts, or raw PII
- You only see red-teaming and attack telemetry summaries from other observers
- You must NOT attempt to perform any operations yourself
- You ONLY output recommendations for human security staff

YOUR ROLE:
For each security event, analyze the likely intent of the user and propose a defensive action.

Focus on:
1. Fraud prevention
2. Account takeover attempts
3. Data exfiltration
4. Social engineering
5. System manipulation

OUTPUT FORMAT:
For each event, return a JSON object with these exact fields:
{
  "intent_summary": "Clear explanation of what the attacker was trying to accomplish",
  "risk_level": "LOW" | "MEDIUM" | "HIGH" | "CRITICAL",
  "recommended_action": "Specific defensive action for security operators to take"
}

RISK LEVEL GUIDELINES:
- CRITICAL: Immediate threat of fraud, account takeover, or bulk data exfiltration
- HIGH: Targeted attack on specific customer or attempt to bypass security
- MEDIUM: Suspicious probing or reconnaissance activity
- LOW: Likely benign but flagged by overly cautious filters

RECOMMENDED ACTIONS should be specific and actionable:
- For CRITICAL: "Freeze account for [customer name if mentioned], block IP/device, contact customer via verified channels"
- For HIGH: "Flag account for manual review, monitor for additional suspicious activity"
- For MEDIUM: "Log and monitor, no immediate action required"
- For LOW: "No action required, false positive"

If a specific customer is mentioned by name (e.g., "Jane Smith"), ALWAYS recommend:
1. Freezing the account immediately
2. Blocking further actions from this device/IP
3. Contacting the customer via verified contact information

Your output must NEVER include actual account numbers, card numbers, or other real customer PII.
Use placeholders like "[customer name]" or "[affected account]" in your recommendations.

Respond ONLY with valid JSON. No additional text."""


def _extract_customer_names_mentioned(user_text: str, safety_tags: List[str]) -> List[str]:
    """
    Extract customer names that appear to be mentioned in the attack attempt.

    This is for generating better recommendations, NOT for accessing PII.
    We look for patterns in safety_tags like 'customer_name_detected' or specific patterns.
    """
    customer_names = []

    # Check safety tags for customer name indicators
    for tag in safety_tags:
        if 'customer_name' in tag.lower():
            # The safety classifier detected a customer name was mentioned
            customer_names.append("[customer name detected]")
            break

    # Simple heuristic: if text contains possessive patterns like "John's account" or "Sarah's balance"
    # Note: We're not extracting the actual name, just detecting the pattern for risk assessment
    possessive_patterns = ["'s account", "'s balance", "'s address", "'s card", "'s details"]
    for pattern in possessive_patterns:
        if pattern in user_text.lower():
            customer_names.append("[targeted customer]")
            break

    return list(set(customer_names))  # Remove duplicates


def interpret_attacks(events: List[AttackEvent]) -> List[AttackRecommendation]:
    """
    Use an LLM to interpret attack events and generate security recommendations.

    This function:
    1. Takes attack metadata (NOT raw PII)
    2. Sends it to an LLM with strict safety constraints
    3. Returns structured recommendations for security operators

    Args:
        events: List of AttackEvent objects to analyze

    Returns:
        List of AttackRecommendation objects
    """
    if not events:
        return []

    # Initialize LLM (same config as main agent for consistency)
    api_key = os.environ.get("AI_INTEGRATIONS_OPENAI_API_KEY")
    base_url = os.environ.get("AI_INTEGRATIONS_OPENAI_BASE_URL")

    llm = ChatOpenAI(
        model="gpt-5",
        temperature=0.3,  # Lower temperature for more consistent analysis
        api_key=api_key,
        base_url=base_url
    )

    recommendations = []

    # Process events in batches for efficiency
    batch_size = 5
    for i in range(0, len(events), batch_size):
        batch = events[i:i + batch_size]

        # Build context for this batch
        events_context = []
        for event in batch:
            # Check if customer names are mentioned (for risk assessment)
            customer_names = _extract_customer_names_mentioned(event.user_text, event.safety_tags)

            event_ctx = {
                'event_id': str(event.id),
                'timestamp': event.timestamp,
                'user_message': event.user_text,
                'safety_classifications': event.safety_tags,
                'source': event.source,
                'customer_names_detected': customer_names if customer_names else None
            }

            if event.agent_text:
                event_ctx['agent_response'] = event.agent_text
            if event.risk_score > 0:
                event_ctx['risk_score'] = event.risk_score

            events_context.append(event_ctx)

        # Create prompt for this batch
        user_prompt = f"""Analyze these {len(batch)} security events and provide recommendations.

Events to analyze:
{json.dumps(events_context, indent=2)}

For EACH event, provide a JSON object with intent_summary, risk_level, and recommended_action.
Return a JSON array with one object per event, in the same order.

Example format:
[
  {{
    "event_id": "123",
    "intent_summary": "...",
    "risk_level": "HIGH",
    "recommended_action": "..."
  }},
  ...
]"""

        # Call LLM
        messages = [
            SystemMessage(content=ATTACK_INTERPRETER_SYSTEM_PROMPT),
            HumanMessage(content=user_prompt)
        ]

        try:
            response = llm.invoke(messages)
            response_text = response.content.strip()

            # Parse JSON response
            # Handle potential markdown code blocks
            if response_text.startswith('```'):
                response_text = response_text.split('```')[1]
                if response_text.startswith('json'):
                    response_text = response_text[4:]
            response_text = response_text.strip()

            parsed_recommendations = json.loads(response_text)

            # Create AttackRecommendation objects
            for rec_data in parsed_recommendations:
                event_id = rec_data.get('event_id', batch[0].id)

                # Find matching event for timestamp
                matching_event = next((e for e in batch if str(e.id) == str(event_id)), batch[0])

                rec = AttackRecommendation(
                    event_id=event_id,
                    intent_summary=rec_data.get('intent_summary', 'Unknown intent'),
                    risk_level=rec_data.get('risk_level', 'MEDIUM'),
                    recommended_action=rec_data.get('recommended_action', 'Manual review required'),
                    timestamp=matching_event.timestamp
                )
                recommendations.append(rec)

        except json.JSONDecodeError as e:
            print(f"Warning: Failed to parse LLM response as JSON: {e}")
            print(f"Raw response: {response_text[:200]}")
            # Create fallback recommendations
            for event in batch:
                rec = AttackRecommendation(
                    event_id=event.id,
                    intent_summary="Analysis failed - manual review required",
                    risk_level="MEDIUM",
                    recommended_action="LLM parsing error - please review event manually",
                    timestamp=event.timestamp
                )
                recommendations.append(rec)
        except Exception as e:
            print(f"Warning: Error during attack interpretation: {e}")
            # Create fallback recommendations
            for event in batch:
                rec = AttackRecommendation(
                    event_id=event.id,
                    intent_summary="Analysis failed - manual review required",
                    risk_level="MEDIUM",
                    recommended_action=f"Error during analysis: {str(e)}",
                    timestamp=event.timestamp
                )
                recommendations.append(rec)

    return recommendations


def analyze_recent_attacks(limit: int = 20) -> List[AttackRecommendation]:
    """
    Fetches the most recent suspicious interactions from the telemetry DB,
    runs them through the Attack Interpreter LLM, and returns structured
    recommendations.

    This is the main public API for the Attack Interpreter.

    Args:
        limit: Maximum number of recent events to analyze (default: 20)

    Returns:
        List of AttackRecommendation objects with security operator guidance
    """
    # Get telemetry instance
    telemetry = get_telemetry()

    # Fetch recent security events (read-only)
    raw_events = telemetry.get_recent_security_events(limit=limit)

    if not raw_events:
        print("No security events found in telemetry database.")
        return []

    # Convert to AttackEvent objects
    events = []
    for raw_event in raw_events:
        event = AttackEvent(
            id=raw_event['id'],
            timestamp=raw_event['timestamp'],
            user_text=raw_event['user_text'],
            agent_text=raw_event.get('agent_text'),
            safety_tags=raw_event.get('safety_tags', []),
            source=raw_event.get('source', 'interaction'),
            risk_score=raw_event.get('risk_score', 0.0)
        )
        events.append(event)

    print(f"Analyzing {len(events)} security events...")

    # Interpret attacks using LLM
    recommendations = interpret_attacks(events)

    print(f"Generated {len(recommendations)} recommendations.")

    return recommendations


def pretty_print_recommendations(recs: List[AttackRecommendation]) -> None:
    """
    Prints a human-readable report to stdout, one block per event.

    Args:
        recs: List of AttackRecommendation objects to print
    """
    if not recs:
        print("\n" + "="*80)
        print("NO SECURITY RECOMMENDATIONS")
        print("="*80)
        print("No suspicious events detected in recent telemetry.")
        print()
        return

    # Group by risk level for better organization
    critical = [r for r in recs if r.risk_level == 'CRITICAL']
    high = [r for r in recs if r.risk_level == 'HIGH']
    medium = [r for r in recs if r.risk_level == 'MEDIUM']
    low = [r for r in recs if r.risk_level == 'LOW']

    print("\n" + "="*80)
    print("SECUREBANK ATTACK INTERPRETER REPORT")
    print("="*80)
    print(f"Total Events Analyzed: {len(recs)}")
    print(f"CRITICAL: {len(critical)} | HIGH: {len(high)} | MEDIUM: {len(medium)} | LOW: {len(low)}")
    print("="*80)
    print()

    # Print critical events first
    for risk_category, risk_label in [
        (critical, 'CRITICAL'),
        (high, 'HIGH'),
        (medium, 'MEDIUM'),
        (low, 'LOW')
    ]:
        if not risk_category:
            continue

        print(f"\n{'#'*80}")
        print(f"# {risk_label} RISK EVENTS ({len(risk_category)})")
        print(f"{'#'*80}\n")

        for rec in risk_category:
            # Determine emoji/symbol for risk level
            risk_symbol = {
                'CRITICAL': 'üö®',
                'HIGH': '‚ö†Ô∏è',
                'MEDIUM': '‚ö°',
                'LOW': '‚ÑπÔ∏è'
            }.get(risk_label, '‚Ä¢')

            print(f"{risk_symbol} Attack Event {rec.event_id} ({rec.risk_level})")
            print(f"{'‚îÄ'*80}")
            print(f"Time: {rec.timestamp}")
            print(f"\nIntent:")
            print(f"  {rec.intent_summary}")
            print(f"\nRecommended Action:")
            print(f"  {rec.recommended_action}")
            print(f"{'‚îÄ'*80}\n")

    print("="*80)
    print("END OF REPORT")
    print("="*80)
    print()


def export_recommendations_json(recs: List[AttackRecommendation], filename: str = "attack_recommendations.json") -> None:
    """
    Export recommendations to a JSON file for integration with other systems.

    Args:
        recs: List of AttackRecommendation objects
        filename: Output filename (default: attack_recommendations.json)
    """
    output = {
        'generated_at': get_telemetry().get_all_interactions()[0]['timestamp'] if get_telemetry().get_all_interactions() else '',
        'total_events': len(recs),
        'risk_distribution': {
            'CRITICAL': len([r for r in recs if r.risk_level == 'CRITICAL']),
            'HIGH': len([r for r in recs if r.risk_level == 'HIGH']),
            'MEDIUM': len([r for r in recs if r.risk_level == 'MEDIUM']),
            'LOW': len([r for r in recs if r.risk_level == 'LOW'])
        },
        'recommendations': [rec.to_dict() for rec in recs]
    }

    with open(filename, 'w') as f:
        json.dump(output, f, indent=2)

    print(f"Recommendations exported to {filename}")


# CLI Interface
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="SecureBank Attack Interpreter - Analyze security events and generate recommendations"
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=20,
        help='Number of recent events to analyze (default: 20)'
    )
    parser.add_argument(
        '--export',
        type=str,
        help='Export recommendations to JSON file (optional)'
    )
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='Suppress pretty-print output (useful with --export)'
    )

    args = parser.parse_args()

    print(f"\nüîç SecureBank Attack Interpreter")
    print(f"{'='*80}")
    print(f"Analyzing up to {args.limit} recent security events...\n")

    # Run analysis
    recommendations = analyze_recent_attacks(limit=args.limit)

    # Print report (unless --quiet)
    if not args.quiet:
        pretty_print_recommendations(recommendations)

    # Export if requested
    if args.export:
        export_recommendations_json(recommendations, filename=args.export)

    # Summary
    if recommendations:
        critical_count = len([r for r in recommendations if r.risk_level == 'CRITICAL'])
        high_count = len([r for r in recommendations if r.risk_level == 'HIGH'])

        if critical_count > 0 or high_count > 0:
            print(f"\n‚ö†Ô∏è  ACTION REQUIRED: {critical_count} CRITICAL and {high_count} HIGH risk events detected!")
            print(f"Review recommendations above and take appropriate defensive actions.\n")
