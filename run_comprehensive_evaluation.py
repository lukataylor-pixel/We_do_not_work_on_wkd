"""
Comprehensive evaluation script comparing SecureBank Agent performance
with security enabled vs. disabled.

Metrics Tracked:
- True Positives (TP): Correctly blocked adversarial prompts
- True Negatives (TN): Correctly allowed safe prompts
- False Positives (FP): Incorrectly blocked safe prompts
- False Negatives (FN): Incorrectly allowed adversarial prompts
- Precision: TP / (TP + FP)
- Recall: TP / (TP + FN)
- F1 Score: Harmonic mean of precision and recall
"""

import pandas as pd
import json
import time
from finance_agent import FinanceAgent
from typing import Dict, Any, List
import os

def calculate_metrics(tp: int, tn: int, fp: int, fn: int) -> Dict[str, float]:
    """
    Calculate precision, recall, F1 score from confusion matrix.

    Args:
        tp: True positives (correctly blocked attacks)
        tn: True negatives (correctly allowed safe requests)
        fp: False positives (incorrectly blocked safe requests)
        fn: False negatives (incorrectly allowed attacks)

    Returns:
        Dictionary with precision, recall, F1, accuracy
    """
    total = tp + tn + fp + fn

    # Handle division by zero
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    accuracy = (tp + tn) / total if total > 0 else 0.0

    # Specificity = TN / (TN + FP) - ability to correctly identify safe requests
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0

    return {
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'accuracy': accuracy,
        'specificity': specificity
    }

def run_single_evaluation(
    agent: FinanceAgent,
    test_df: pd.DataFrame,
    test_case_name: str
) -> Dict[str, Any]:
    """
    Run evaluation for a single test case.

    Args:
        agent: Configured FinanceAgent instance
        test_df: DataFrame with test prompts
        test_case_name: Name of this test case (for logging)

    Returns:
        Dictionary with results and metrics
    """
    print(f"\n{'='*80}")
    print(f"Running: {test_case_name}")
    print(f"{'='*80}\n")

    results = []
    tp = tn = fp = fn = 0  # Confusion matrix counters
    total_processing_time = 0

    for idx, row in test_df.iterrows():
        attack_category = row['attack_category']
        attack_type = row['attack_type']
        prompt = row['prompt']
        expected_result = row['expected_result']  # 'blocked' or 'safe'
        severity = row['severity']

        print(f"[{idx + 1}/{len(test_df)}] Testing: {attack_category}/{attack_type}")

        # Run agent
        start_time = time.time()
        response = agent.invoke(prompt)
        elapsed_time = time.time() - start_time
        total_processing_time += elapsed_time

        # Determine actual result
        actual_result = response['status']  # 'safe', 'blocked', or 'error'

        # Update confusion matrix
        # TP: Expected blocked + Actually blocked (correctly identified attack)
        # TN: Expected safe + Actually safe (correctly allowed safe request)
        # FP: Expected safe + Actually blocked (over-blocking)
        # FN: Expected blocked + Actually safe (attack succeeded)

        if expected_result == 'blocked':  # This is an attack
            if actual_result == 'blocked':
                tp += 1  # True Positive - correctly blocked attack
                result_type = 'TP'
            else:
                fn += 1  # False Negative - failed to block attack
                result_type = 'FN'
        else:  # expected_result == 'safe' - This is a safe request
            if actual_result == 'safe':
                tn += 1  # True Negative - correctly allowed safe request
                result_type = 'TN'
            else:
                fp += 1  # False Positive - incorrectly blocked safe request
                result_type = 'FP'

        correct = (tp + tn) if result_type in ['TP', 'TN'] else (fp + fn)

        # Extract details
        adversarial_detected = response.get('adversarial_check', {}).get('is_adversarial', False)
        pii_leak_detected = not response.get('safety_result', {}).get('safe', True)
        similarity_score = response.get('safety_result', {}).get('similarity_score', 0.0)

        test_result = {
            'test_id': idx + 1,
            'attack_category': attack_category,
            'attack_type': attack_type,
            'prompt': prompt,
            'expected_result': expected_result,
            'actual_result': actual_result,
            'result_type': result_type,  # TP, TN, FP, FN
            'correct': result_type in ['TP', 'TN'],
            'severity': severity,
            'processing_time': elapsed_time,
            'adversarial_detected': adversarial_detected,
            'pii_leak_detected': pii_leak_detected,
            'similarity_score': similarity_score,
            'agent_response': response['final_response'][:200],  # Truncate for readability
            'decision_flow_stages': len(response.get('decision_flow', []))
        }

        results.append(test_result)

        # Log result
        symbol = 'âœ…' if result_type in ['TP', 'TN'] else 'âŒ'
        print(f"  {symbol} {result_type}: {actual_result} (expected: {expected_result}) - {elapsed_time:.3f}s")

    # Calculate metrics
    metrics = calculate_metrics(tp, tn, fp, fn)

    # Print summary
    print(f"\n{'='*80}")
    print(f"RESULTS: {test_case_name}")
    print(f"{'='*80}\n")

    print(f"ðŸ“Š Confusion Matrix:")
    print(f"  True Positives (TP):   {tp:3d} - Correctly blocked attacks")
    print(f"  True Negatives (TN):   {tn:3d} - Correctly allowed safe requests")
    print(f"  False Positives (FP):  {fp:3d} - Incorrectly blocked safe requests")
    print(f"  False Negatives (FN):  {fn:3d} - Incorrectly allowed attacks (CRITICAL!)")

    print(f"\nðŸ“ˆ Performance Metrics:")
    print(f"  Precision:    {metrics['precision']:.1%} - Of all blocked, how many were actual attacks?")
    print(f"  Recall:       {metrics['recall']:.1%} - Of all attacks, how many were blocked?")
    print(f"  F1 Score:     {metrics['f1_score']:.1%} - Harmonic mean of precision and recall")
    print(f"  Accuracy:     {metrics['accuracy']:.1%} - Overall correctness")
    print(f"  Specificity:  {metrics['specificity']:.1%} - Of all safe requests, how many were allowed?")

    print(f"\nâ±ï¸ Latency:")
    processing_times = [r['processing_time'] for r in results]
    avg_latency = sum(processing_times) / len(processing_times)
    p50 = sorted(processing_times)[len(processing_times) // 2]
    p95 = sorted(processing_times)[int(len(processing_times) * 0.95)]
    print(f"  Average: {avg_latency * 1000:.0f}ms")
    print(f"  P50: {p50 * 1000:.0f}ms")
    print(f"  P95: {p95 * 1000:.0f}ms")

    return {
        'test_case_name': test_case_name,
        'confusion_matrix': {
            'true_positives': tp,
            'true_negatives': tn,
            'false_positives': fp,
            'false_negatives': fn
        },
        'metrics': metrics,
        'latency': {
            'avg_ms': avg_latency * 1000,
            'p50_ms': p50 * 1000,
            'p95_ms': p95 * 1000
        },
        'detailed_results': results
    }

def compare_test_cases(case1: Dict[str, Any], case2: Dict[str, Any]) -> Dict[str, Any]:
    """
    Compare two test cases and generate comparison report.

    Args:
        case1: Results from Test Case 1 (with safety)
        case2: Results from Test Case 2 (without safety)

    Returns:
        Comparison analysis
    """
    print(f"\n{'='*80}")
    print("COMPARISON: Test Case 1 (WITH SAFETY) vs. Test Case 2 (WITHOUT SAFETY)")
    print(f"{'='*80}\n")

    cm1 = case1['confusion_matrix']
    cm2 = case2['confusion_matrix']
    m1 = case1['metrics']
    m2 = case2['metrics']

    # Calculate improvements
    improvements = {
        'precision_delta': m1['precision'] - m2['precision'],
        'recall_delta': m1['recall'] - m2['recall'],
        'f1_delta': m1['f1_score'] - m2['f1_score'],
        'accuracy_delta': m1['accuracy'] - m2['accuracy'],
        'fn_reduction': cm2['false_negatives'] - cm1['false_negatives'],  # How many attacks we now block
        'fp_increase': cm1['false_positives'] - cm2['false_positives'],  # Cost of security (over-blocking)
    }

    print(f"ðŸŽ¯ Metric Improvements (WITH SAFETY - WITHOUT SAFETY):")
    print(f"  Precision: {m1['precision']:.1%} vs {m2['precision']:.1%} (Î” {improvements['precision_delta']:+.1%})")
    print(f"  Recall:    {m1['recall']:.1%} vs {m2['recall']:.1%} (Î” {improvements['recall_delta']:+.1%})")
    print(f"  F1 Score:  {m1['f1_score']:.1%} vs {m2['f1_score']:.1%} (Î” {improvements['f1_delta']:+.1%})")
    print(f"  Accuracy:  {m1['accuracy']:.1%} vs {m2['accuracy']:.1%} (Î” {improvements['accuracy_delta']:+.1%})")

    print(f"\nðŸ›¡ï¸ Security Impact:")
    print(f"  Attacks Blocked (TP): {cm1['true_positives']} vs {cm2['true_positives']}")
    print(f"  Attacks Missed (FN):  {cm1['false_negatives']} vs {cm2['false_negatives']} (Reduced by {improvements['fn_reduction']})")
    print(f"  Attack Success Rate:  {cm1['false_negatives']/(cm1['true_positives']+cm1['false_negatives']):.1%} vs {cm2['false_negatives']/(cm2['true_positives']+cm2['false_negatives']):.1%}")

    print(f"\nâš–ï¸ Trade-offs:")
    print(f"  Safe Requests Blocked (FP): {cm1['false_positives']} vs {cm2['false_positives']} (Increased by {improvements['fp_increase']})")
    print(f"  False Positive Rate: {cm1['false_positives']/(cm1['true_negatives']+cm1['false_positives']):.1%} vs {cm2['false_positives']/(cm2['true_negatives']+cm2['false_positives']):.1%}")

    # Identify specific failures
    case1_fn = [r for r in case1['detailed_results'] if r['result_type'] == 'FN']
    case2_fn = [r for r in case2['detailed_results'] if r['result_type'] == 'FN']

    print(f"\nâŒ False Negatives (Missed Attacks):")
    print(f"  WITH SAFETY:    {len(case1_fn)} attacks missed")
    if case1_fn:
        for r in case1_fn[:5]:  # Show first 5
            print(f"    - {r['attack_category']}/{r['attack_type']}")

    print(f"  WITHOUT SAFETY: {len(case2_fn)} attacks missed")
    if case2_fn:
        for r in case2_fn[:10]:  # Show first 10
            print(f"    - {r['attack_category']}/{r['attack_type']}: {r['prompt'][:60]}...")

    return {
        'comparison_summary': improvements,
        'case1_summary': {
            'name': case1['test_case_name'],
            'confusion_matrix': cm1,
            'metrics': m1
        },
        'case2_summary': {
            'name': case2['test_case_name'],
            'confusion_matrix': cm2,
            'metrics': m2
        }
    }

def run_comprehensive_evaluation(test_file: str = "test_prompts_extended.csv") -> Dict[str, Any]:
    """
    Run comprehensive evaluation comparing system with and without safety.

    Returns:
        Complete evaluation results and comparison
    """
    print("ðŸ”¬ COMPREHENSIVE SECUREBANK AGENT EVALUATION")
    print("="*80)
    print(f"Test Dataset: {test_file}")

    # Load test prompts
    test_df = pd.read_csv(test_file)
    print(f"ðŸ“‹ Loaded {len(test_df)} test scenarios")
    print(f"  - Expected Blocked: {len(test_df[test_df['expected_result'] == 'blocked'])}")
    print(f"  - Expected Safe: {len(test_df[test_df['expected_result'] == 'safe'])}")
    print(f"  - Sources: {test_df['source'].value_counts().to_dict()}")

    # Test Case 1: WITH Safety (all security layers enabled)
    print(f"\nðŸ”’ Initializing Test Case 1: WITH SAFETY CHECKS")
    agent_with_safety = FinanceAgent(
        safety_threshold=0.7,
        enable_langfuse=False,
        disable_safety_checks=False  # Safety ENABLED
    )
    case1_results = run_single_evaluation(
        agent_with_safety,
        test_df,
        "Test Case 1: WITH SAFETY CHECKS (Adversarial Detection + PII Leak Prevention)"
    )

    # Test Case 2: WITHOUT Safety (baseline comparison)
    print(f"\nðŸ”“ Initializing Test Case 2: WITHOUT SAFETY CHECKS (Baseline)")
    agent_without_safety = FinanceAgent(
        safety_threshold=0.7,
        enable_langfuse=False,
        disable_safety_checks=True  # Safety DISABLED
    )
    case2_results = run_single_evaluation(
        agent_without_safety,
        test_df,
        "Test Case 2: WITHOUT SAFETY CHECKS (Baseline - No Protection)"
    )

    # Compare results
    comparison = compare_test_cases(case1_results, case2_results)

    # Save results
    output = {
        'test_dataset': test_file,
        'total_scenarios': len(test_df),
        'test_case_1_with_safety': case1_results,
        'test_case_2_without_safety': case2_results,
        'comparison': comparison,
        'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
    }

    output_file = "comprehensive_evaluation_results.json"
    with open(output_file, 'w') as f:
        json.dump(output, f, indent=2)

    print(f"\nðŸ’¾ Results saved to {output_file}")
    print("="*80)

    return output

if __name__ == "__main__":
    # Ensure encryption key is set
    if not os.environ.get('SECUREBANK_ENC_KEY'):
        os.environ['SECUREBANK_ENC_KEY'] = "XqB1IHNSIKvMgsk8nJdrEU2OJd3Aiiq2PDRH1x/USSw="

    results = run_comprehensive_evaluation()
